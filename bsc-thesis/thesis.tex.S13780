 \documentclass[oneside,a4paper,12pt]{book}
%\pagestyle{headings}
\usepackage{enumitem}
\usepackage{listings}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{tikz} % To generate the plot from csv
\usepackage{pgfplots}
\usepackage{float}
\pgfplotsset{compat=newest} % Allows to place the legend below plot
\usepgfplotslibrary{units} % Allows to enter the units nicely
\frontmatter
\sisetup{
	round-mode          = places,
	round-precision     = 2,
}

\input{preamble}
\lstdefinestyle{base}{
	emptylines=1,
	breaklines=true,
	escapeinside={\%*}{*)}, 
	basicstyle=\ttfamily\color{black},
	moredelim=**[is][\color{orange}]{@}{@},
	moredelim=**[is][\color{blue}]{"}{"},
	moredelim=**[is][\color{green}]{ç}{ç},
}
% A B S T R A C T
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter*{\centering Abstract}
\begin{quotation}
\noindent 
For the Computer to use and understand data it is necessary to parse it into computer readable format. For building a parser a model of the grammar and structure of the data must be known or created. For many new programming language dialects and also for log files of new software these models are normally not available. For creating a model, the structure and grammar information must therefore be inferred from the source code which is a time consuming process. The goal of this project is helping developers find structure of software code automatically.
About Results!
\end{quotation}
\clearpage


% C O N T E N T S 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\tableofcontents

\mainmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% NEW CHAPTER %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{cha:introduction}
\section{Motivation}
Writing software means reading software code. For developers it is crucial to understand the state of the system in order to carry out development and software evolution tasks. As a consequence, developers often use as much time reading and understanding the written code as actually writing new code. Analysing tolls for integrated development environments can help developers understanding code and reduce time for assessing software code.\cite{nierstrasz2012agile}  For that such tools can be created a software model of the used language must exist in form of a parser. For many dialects of programming language these models do not exists. Aside from programming languages log files are a very important part of software applications. Many current software applications do produce auxiliary text files as output. These log files are used various ways for example in debugging and profiling purposes. While generating log files is a very simple and straightforward process, the understanding of log files can be quite hard, as these can be very large files with complex structure.\cite{valdman2001log}  If a model of the grammar und underlying structure does not exist, it is necessary to infer these properties from the source code. The inferring of a software model and the building of a custom parser is a complex task that can take several people several months to accomplish. 
In this bachelors project we tried to build a tool that can help distinguish different pattern and structures automatically from software code or log files.\cite{dubey2006inferring}
\section{Goal and Focus}
The goal of this thesis is to develop a software tool that can atomically find structure in software code od unknown software code or log files and to generate different sets consisting of related pattern so that a parser rule can be inferred. The process of creating the software tool manifested three different sub-problems that had to be addressed:

\begin{enumerate}[ label=\textbf{\arabic*}.]
	
	\item \textbf{Statement creation.} Structure manifests itself in an ordered sequence of different tokens. For finding different patterns, a smallest statement had to be found, which is able to hold structural information. For this thesis it is assumed, that carriage returns are a natural and often used way which indicates the beginning of a new pattern. A statement is then split into tokens. A token consists of one or more Symbols and in is defined through first the encapsulating of blank spaces and second in a change of symbols.


	\item \textbf{Representation and differentiation.} For algorithmically deciding and quantifying the difference between statements a way for mathematically representing needs to be introduced. The approaches used in these methods differ from creating a vector out of defining features. but instead focuses on the type of symbols used in a token.

	\item \textbf{Clustering.} A programming language consists of a finite number of different pattern that are used for describing a Software. These pattern are therefore used multiple times only differing in the used variables and parameters. Patterns that are similar or differ only in used variables must be filtered. Filtering is achieved through k-means clustering. Patterns should be clustered according to their similarity.
\end{enumerate}
To achieve a solution to these sub-problems, certain assumptions about the commonalities of languages and log files had to be made. We tried to keep these assumptions as minimal as possible so that a wide variety of programming languages and log files could be analysed:

\begin{itemize}
	\item 	A Language consists of a finite number of patterns that differ only in used variables and parameters.
	\item Software Code contains indents and carriage returns as is best practice for many programming languages amongst developers.
\end{itemize}

\chapter {Methods}
In this chapter we describe in detail the used methods for creating and tokenizing token and put forward the different representations used in the created tool. In our context a method defines a Process if replaced would result in a different output of the tool. For visualising our methods, we use the following simplified Java example: 

\lstset{
	basicstyle=\small\sffamily,
	numbers=left,
	numberstyle=\tiny,
	frame=tb,
	columns=fullflexible,
	showstringspaces=false
	moredelim=**[is][\color{red}]{@}{@},
}
\begin{center}
\begin{lstlisting}[caption=Simplified Java Code, style = base,
label=a_label,]
if (x > 1) {return 0;}
if (x <= 0) {return 1;}
else {return v1;}
\end{lstlisting}
\end{center}

\section{Statements and Tokens}
To be able to cluster the data, it first needs to be processed into smaller instances that are comparable to each other. As mentioned in the introduction, we used in this project different levels of instances called: Statements and tokens, where a Statement consists of tokens. A statement can be created in different ways. Because the main effort of this thesis lies in the structural clustering with the k-means algorithm, the creation of statements is simplified.
\newline
\newline
\textbf{A statement is a line of code.} Carriage retunes and line feeds are a very important way to make software code readable, creating a structure the human eye can perceive more easily. The way of using carriage returns and line feeds for defining a statements tries to exploit the way we make software code and log files more readable for humans.
\newline
\newline
In our example before, each new line, would become a statement, creating three different statements from this code snipped:
\begin{center}
\begin{lstlisting}[caption=Statements in Java example, style = base,
	label=a_label,]
	if (x > 1) {return 0;}
	if (x <= 0) {return 1;}
	else {return v1;}
\end{lstlisting}
\end{center}
\textbf{A token is a single word, numeral or punctuation.} A statement is further processed into tokens. The smallest instance. Words are a sequence of at least one letter and can contain numerals. \textbf{Words} cannot contain punctuation. \textbf{Numerals} are a sequence of at least one digit. A Numeral cannot contain letters or punctuation. \textbf{Punctuation} is one special character which cannot be a letter nor a digit.
\newline
\newline
In our example tokens would be made as follows:

\begin{center}
\begin{lstlisting}[caption=Tokens in statements, style = base,
	label=tokens,]
	@if@ "(" @x@ ">" ç1ç) "{"@return@ ç0ç";""}" 
	@if@ "("@x@ "<""=" ç0ç")" {@return@ ç1ç";""}"
	@else@ "{"@return@ @v1@";""}"
	
\end{lstlisting}
\end{center}
In this example the words are marked in orange, numerals are in green and punctuation is in blue. Tokens are divided by spaces. Punctuation that is not encapsulated by spaces will still be tokenized as an individual special. This means as seen in the example, that a “<=” will be tokenized into two tokens: “<” and “=”.  Variables such as “v1” are tokenized as a word.

\section{Representors}
As mentioned in the introduction, vector representation is needed for clustering statements. A Representor adds or modifies representation to a statement. There are three different categories of Representors: Type Representors, Distance Representors and Weight Representors.
\begin{enumerate}[ label=\textbf{\arabic*}.]
	
		\item \textbf{Type Representor.} Type Representors create vector representations according to the characteristics of the tokens of the statement and assign numbers to the vector accordingly.
	
	
		\item \textbf{Distance Representor.} Distance Representors calculate distance information from the given representation of a Type Representor.
	
		\item \textbf{Weight Representor.} Weight Representors function as noise cancelling or filtering and can weight specific parts of a representation more or less.

\end{enumerate}
Representors can be concatenated in the order above. A Type Representor is always needed, but can optionally be followed by a Distance Representor or a Weight Representor or both.
\subsection{Type Structure Representor}
The first Type Representor to be looked at is the Type Structure Representor. It assigns a strict value according to the type of the token. \newline
\newline \newline
Example: In our implementation we assigned the values 0 for words, 1 for numerals and 2 for Punctuation. For our previous used Java snipped this gives us the vector representations:

\begin{lstlisting}[caption=Java snipped, style = base,
label=a_label,]
@if@ "("@x@ ">" ç1ç")" "{"@return@ ç0ç";}" 
@if@ "("@x@ "<=" ç0ç")" "{"@return@ ç1ç";}" 
@else@ "{"@return@ @v1@";}"    
\end{lstlisting}
\begin{lstlisting}[caption=Type Structure Representation example as vectors, style = base,
label=lst:type structure representation,]
(@0@ "2" @0@ "2" ç1ç "2 2" @0@ ç1ç "2 2" )
(@0@ "2" @0@ "2 2" ç1ç "2 2" @0@ ç1ç "2 2")
(@0@ "2" @0 0@ "2 2")
\end{lstlisting}
\subsection{Type Specific Representor}
The Type Specific Representor uses the same structural information for calculating the vector representation as the Type Structure Representor. But instead of assigning a single value per type, the Type Specific Representor assigns a value per encountered unique token, with the exception of bracket symbols, for which opening and closing brackets are treated as one unique token. To maintain the structural information, the assigned value of the token is then normed by the type of the token. A vector element therefore is defined as:\newline $\displaystyle \centering vector\_element = \frac{1}{assigned\_token\_value} + type\_value$.\newline 


Example: Same as in the Type Structure Representor we assigned the values 0 for words, 1 for numerals and 2 for Punctuation. A punctuation token therefore would always be in the interval ${( 2,3 ]}$.
\newline \newline For our previous used Java snipped this gives us the vector representations:
\begin{lstlisting}[caption=Type Specific Representation example, style = base,
label=a_label,]
@if@ "("@x@ ">" ç1ç")" "{"@return@ ç0ç";}"
@if@ "("@x@ "<=" ç0ç")" "{"@return@ ç1ç";}"
@else@ "{"@return@ @v1@";}"
\end{lstlisting}
\begin{lstlisting}[caption=Type Specific Representation as vectors example, style = base,
label=a_label,]
(%*\textcolor{orange}{$\frac{1}{1}+0$} \textcolor{blue}{$\frac{1}{2}+2$}  \textcolor{orange}{$\frac{1}{3}+0$} \textcolor{blue}{$\frac{1}{4}+2$} \textcolor{green}{$\frac{1}{5}+1$} \textcolor{blue}{$\frac{1}{2}+2$} \textcolor{blue}{$\frac{1}{6}+2$} \textcolor{orange}{$\frac{1}{7}+0$} \textcolor{green}{$\frac{1}{8}+1$} \textcolor{blue}{$\frac{1}{9}+2$} \textcolor{blue}{$\frac{1}{6}+2$}*) )
(%*\textcolor{orange}{$\frac{1}{1}+0$} \textcolor{blue}{$\frac{1}{2}+2$}  \textcolor{orange}{$\frac{1}{3}+0$} \textcolor{blue}{$\frac{1}{4}+2$} \textcolor{blue}{$\frac{1}{10}+2$} \textcolor{green}{$\frac{1}{8}+1$} \textcolor{blue}{$\frac{1}{2}+2$} \textcolor{blue}{$\frac{1}{6}+2$} \textcolor{orange}{$\frac{1}{7}+0$} \textcolor{green}{$\frac{1}{8}+2$} \textcolor{blue}{$\frac{1}{9}+2$} \textcolor{blue}{$\frac{1}{6}+2$}*) )
(%*\textcolor{orange}{$\frac{1}{11}+0$}  \textcolor{blue}{$\frac{1}{6}+2$} \textcolor{orange}{$\frac{1}{7}+0$} \textcolor{orange}{$\frac{1}{12}+0$} \textcolor{blue}{$\frac{1}{9}+2$} \textcolor{blue}{$\frac{1}{6}+2$}*) )
\end{lstlisting}
\subsection{Distance Zero Representor}
For exploiting distance information, the Distance Zero Representor measures the distance between the occurrences of the same values given by the used Type Representor. Measurement starts with zero as a new value is first encountered. If the same value is encountered after that, the new vector element will be the distance between the first encounter and the current position of the vector element.\newline\newline
Example: For simplifying the example, Type Structure Representor is used for assigning type values.
\begin{lstlisting}[caption=Type Representation as calculated in Listing \ref{lst:type structure representation}, style = base,
label=a,]
(@0@ "2" @0@ "2" ç1ç "2 2" @0@ ç1ç "2 2" )
(@0@ "2" @0@ "2 2" ç1ç "2 2" @0@ ç1ç "2 2")
(@0@ "2" @0 0@ "2 2")
\end{lstlisting}
\begin{lstlisting}[caption=Distance Zero Representation of Type Structure Representation on Listing \ref{a}, style = base,
label=a,]
(@0@ "0" @0@ "2" ç4ç "4 5" @5@ ç8ç "8 9" )
(@0@ "0" @0@ "2 3" ç5ç "5 6" @6@ ç9ç "9 10")
(@0@ "0" @2 3@ "3 4")
\end{lstlisting}
\subsection{Distance All Representor}
The Distance All Representor calculates the maximal distance between to previously assigned type values given by a Type Representor. It then sets the calculated distance for all values of the same previously assigned type value.\newline \newline

Example:
\subsection{Weight Summary Representor}
\subsection{Weight Sum First Representor}
\subsection{Weight Inverse Representor}

\section{K-Means}
K-means is an unsupervised clustering algorithm. It clusters the given data in k different clusters according to the vector representation of the data. For clustering the data k different centroids are randomly initialized and the algorithm labels the data according to the nearest centroid. At the end of the algorithm the centroids will have minimal distance to its assigned data points. It achieves this by moving the centroids to mathematical mean of all the assigned data points. The mathematical expression, given the trainings set $\{x^{(1)},..., x^{(m)}\}$ and $x^{(i)} \in \mathbb{R}$ of the k-means algorithm is as follows:\newline

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{kmeansformula}
	\caption{Mathematical expression of k-means \cite{ngcs229}}
\end{figure}
A practical example of the k-means algorithm:\newline

	\begin{figure}[H]
		\centering
		\includegraphics[width=1\textwidth]{kmeansexample}
		\caption{Example of k-means iteration \cite{ngcs229}}
	\end{figure}
The red and blue cross are the centroids and are initialized randomly. The centroids will then gradually move nearer to the mean of the final clusters for each iteration, minimizing the cost or distortion function:
\newline
\begin{center}
	$\displaystyle  J(c,\mu) = \sum_{i=1}^{m} ||x^{(i)} - \mu_{c^{(i)}}||^2$.
\end{center}
$J$ measures the sum of squared distances between each training example $x^{(i)}$ and the centroid of the cluster $\mu_{c^{(i)}}$ to which it has been assigned.
\section{The Elbow Method}
One problem of the k-means algorithm is that the parameter k must be known. In our case the parameter k is not known. K is dependent on the given input; on which we want to limit our assumptions. Therefore, a way to algorithmically calculate k is needed. The idea behind the Elbow Method is to find the number of centroids, so that you have the maximal number of centroids, which is k, for which the fit of the centroids to the data - measured by the cost function J-  does not improve significantly any more. In the case of k equals one, the position of the centroid would be the mean of all data point and in the case of k equals the number of date point the centroids would be the points themselves, giving for k equals one the worst fit and for k equals the number of data points a perfect fit. This can be illustrated by the following example where we normed J by k:
\newline
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
		\begin{axis}[
		%width=\linewidth, % Scale the plot to \linewidth
		grid=major, % Display a grid
		grid style={dashed,gray!30}, % Set the style
		xlabel=k, % Set the labels
		ylabel= $ \frac{J(c,\mu ) }{ k} $,
		x unit=, % Set the respective units
		y unit=,
		legend style={at={(0.5,-0.2)},anchor=north}, % Put the legend below the plot
		%x tick label style={rotate=90,anchor=east}, % Display labels sideways
		y label style={rotate=-90}
		]
		\addplot 
		% add a plot from table; you select the columns by using the actual name in
		% the .csv file (on top)
			table[y=J/k,x=k,col sep=comma] {jbyk.csv}; 
				xmin=1,xmax =20,
		\legend{}
		\end{axis}
		\end{tikzpicture}
		\caption{Average Fit per centroid per k}
	\end{center}
\end{figure}
As seen in the figure above, the improvement in the data modelling decreases as k becomes larger. By calculating the partial derivative $\displaystyle \frac{c_{i-1}-c_{i+1}}{k_{i-1} - k_{i+1}}$, this decrease can be measured. For getting the value for k, the partial derivative is maximized until it reaches a threshold $\alpha$, with $ \alpha \in \mathbb{R}^{^-}$.


\chapter {Conclusion and Future Work}
In which we step back, have a critical look at the entire work, then conclude, and learn what lays beyond this thesis.
\bibliography{thesis}
\bibliographystyle{plain}
\lstlistoflistings
\listoffigures
\chapter {Anleitung zu wissenschaftlichen Arbeiten}
This consists of additional documentation, e.g. a tutorial, user guide etc.
Required by the Informatik regulation.

%END Doc
%-------------------------------------------------------



\end{document}